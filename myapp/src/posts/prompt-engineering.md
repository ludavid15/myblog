---
title: "Prompt Engineering"
topic: "Machine Learning"
path: "prompt-engineering"
author: "David Lu"
date: "2024-06-26"
preview: "This article provides tips and tricks for prompting large-language models."
---

# Guidelines for Prompting

<v-divider></v-divider>

This first section outlines a few general tips and tricks for what to include in your model prompt. 

## Write clear and specific instructions

1. **Delimiters** - Don't be afraid here to add more detail. This can be something specific like "summarize the text delimited by triple backticks". This eliminates the possibility that the model listens to something in the prompt (which is what happens when models are subject to a prompt injection attack). 
2. **Define Output Structure** - You can specify the output format, like JSON or XML. This can help you process or use the output. 
3. **Check Edge Cases** - As part of this, you can also include instructions about what to do with edge cases. 
4. **Few Shot Prompting** - Give successful examples of completing tasks before prompting. 

## Give the model time to think

Specify the steps required to complete a task. This helps the model slow down and break a task up into individual steps. This might also help with seeing the logic and reasoning it uses. This is an important step where you can define the rules that the model should follow. 

# Applications

<v-divider></v-divider>

## Summarizing
When tasked to summarize a result, consider specifying the nature of what is most relevant. For example, if summarizing a product review, maybe we just want to extract the pieces about value, and nothing else. You can specify any such preferences in your prompt to get more tailored responses. 

## Inferring
An inferring task can be thought of as something like "analysis" of text. This is pretty easy with LLM's since the old process required a tailored sentiment classification model that needs many more steps with its own training data set. 

## Transforming
This covers tasks like translation, or tone modification, or even as a grammar check. 

## Expanding
These tasks extend some initial script and generate more content. An example use case is to auto-generate responses to customer reviews or questions. For tasks like this, keep in mind the *temperature* of the response. A temperature of 0 will always choose to the most likely response, and is the best way to produce reliable results. For ethical consideration, you should tell your AI bot to add in some text every time clarifying that the result has been produced by AI.

## ChatBot
To construct a ChatBot, you will need a clever data structure to track the conversation while providing instructions to the LLM. One such way is use a list/dictionary like object which includes the following roles:

1. **System** - instructions to the LLM
2. **Assistant** - messages written by the assistant, the next line is to be generated by the LLM, and past messages already expressed. 
3. **User** - the human or entity asking questions to the LLM.

This dictionary will grow as the conversation continues, and is known as the "context". 
